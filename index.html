<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>EnvPoser: Environment-aware Realistic Human Motion Estimation from Sparse Observations with Uncertainty Modeling</title>
    <link rel="icon" type="image/png" sizes="16x16" href="images/favicon_package/android-chrome-512x512.png">

    <link rel="stylesheet" href="css/bootstrap-4.4.1.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="fonts/avenir-next/stylesheet.css">
    <link rel="stylesheet" href="fonts/segoe-print/stylesheet.css">
    <link rel="stylesheet" href="css/window.css">
    <link rel="stylesheet" href="css/carousel.css">
    <link rel="stylesheet" href="css/selection_panel.css">
    <link rel="stylesheet" href="css/main.css">
    <!-- <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet"> -->
    <link rel="stylesheet" href="css/bulma-carousel.min.css">
    <link rel="stylesheet" href="css/bulma-slider.min.css">
    <!-- <link rel="stylesheet" href="css/bulma.min.css"> -->
    <link rel="stylesheet" href="css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="css/index.css">

    <script src="js/window.js"></script>
    <script src="js/carousel.js"></script>
    <script src="js/selection_panel.js"></script>
    <script src="js/generation.js"></script>
    <script src="js/editing.js"></script>
    <script src="js/application.js"></script>
    <script src="js/main.js"></script>
    <script type="module" src="https://unpkg.com/@google/model-viewer/dist/model-viewer.min.js"></script>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="js/fontawesome.all.min.js"></script>
    <script src="js/bulma-carousel.min.js"></script>
    <script src="js/bulma-slider.min.js"></script>
    <script src="js/index.js"></script>
    <script src="js/video_comparison.js" defer></script>
  </head>

  <!-- cover -->
  <section>
    <div class="jumbotron text-center mt-0">
      <div class="container">
        <div class="row">
          <div class="col-12">
            <h2><b><span class="x-gradient-font">EnvPoser</span>: Environment-aware Realistic Human Motion Estimation from Sparse Observations with Uncertainty Modeling</b></h2>
            <h6>
              <a href="https://xspc.github.io/" target="_blank">Songpengcheng Xia</a><sup>1</sup>,
              <a target="_blank">Yu Zhang</a><sup>1</sup>,
              <a href="https://suzhuo.github.io/" target="_blank">Zhuo Su</a><sup>2,&dagger;</sup>,
              <a target="_blank">Xiaozheng Zheng</a><sup>2</sup>,
              <a target="_blank">Zheng Lv</a><sup>2</sup>,
              <a target="_blank">Guidong Wang</a><sup>2</sup>,
	    </h6>
	    <h6>
              <a target="_blank">Yongjie Zhang</a><sup>2</sup>,
	      <a href="https://sjtu-robotics.com/zh/" target="_blank">Qi Wu</a><sup>1</sup>,
	      <a href="https://scholar.google.com.hk/citations?user=HgZ0wNwAAAAJ&hl=zh-CN&oi=ao" target="_blank">Lei Chu</a><sup>1</sup>,
              <a href="hhttps://scholar.google.com.hk/citations?user=Vm7d2EkAAAAJ&hl=zh-CN&oi=ao" target="_blank">Ling Pei</a><sup>1,&Dagger;</sup>
            </h6>
            <p style="text-align: center">
              <sup>1</sup>Shanghai Jiao Tong University &nbsp;&nbsp;
              <sup>2</sup>ByteDance
              <br>
              <i><sup>&dagger;</sup>Project Leader</i>
              <br>
              <i><sup>&Dagger;</sup>Corresponding author</i>
            </p>
            <div class="row justify-content-center">
              <div class="column">
                <p class="mb-5"><a class="btn btn-large btn-light" href="https://arxiv.org/abs/2412.10235" role="button"  target="_blank">
                  <i class="fa fa-file"></i> Paper </a> </p>
              </div>
              <!-- <div class="column">
                <p class="mb-5"><a class="btn btn-large btn-light" href="https://www.youtube.com/watch?v=88_CyBNtEe8&t=168s" role="button"  target="_blank">
                  <i class="fab fa-youtube"></i> Video </a> </p>
              </div> -->
              <div class="column">
                <p class="mb-5"><a class="btn btn-large btn-light" href="https://github.com/xspc/EnvPoser" role="button"  target="_blank">
                  <i class="fab fa-github"></i> Code(Coming soon) </a> </p>
              </div>
            </div>

          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- abstract -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
          <div class="row" style="margin-bottom:5px">
            <div class="col" style="text-align:center">
              <img src="images/teaser.png" width="90%">
            </div>
          </div>
          <div id="abstract" class="x-gradient-block">
            Estimating full-body motion using the tracking signals of head and hands from VR devices holds great potential for various applications. However, the sparsity and unique distribution of observations present a significant challenge, resulting in an ill-posed problem with multiple feasible solutions (i.e., hypotheses). This amplifies uncertainty and ambiguity in full-body motion estimation, especially for the lower-body joints. Therefore, we propose a new method, EnvPoser, that employs a two-stage framework to perform full-body motion estimation using sparse tracking signals and pre-scanned environment from VR devices. EnvPoser models the multi-hypothesis nature of human motion through an uncertainty-aware estimation module in the first stage. In the second stage, we refine these multi-hypothesis estimates by integrating semantic and geometric environmental constraints, ensuring that the final motion estimation aligns realistically with both the environmental context and physical interactions.
Qualitative and quantitative experiments on two public datasets demonstrate that our method achieves state-of-the-art performance, highlighting significant improvements in human motion estimation within motion-environment interaction scenarios. Our code will be released at our project page.
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- pipeline -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
          <div class="x-section-title"><div class="x-gradient-font">Pipeline</div></div>
          <img src="images/pipeline.png" width="90%">
        </div>
      </div>
      <br>
      <p class="text-left">
        Overview of EnvPoser: A Two-Stage Motion Estimation Model. Stage I involves training the uncertainty-aware initial estimation module on the AMASS dataset to produce initial motion estimates with uncertainty quantification. Stage II refines these estimates by training on motion-environment datasets, incorporating semantic and geometric environmental constraints.
      </p>
    </div>
  </section>

  
<!-- results -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
          <div class="x-section-title"><div class="x-gradient-font">Long Narrated Video</div></div>
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item item-steve video-grid1">
              <video poster="" id="steve gt_rgb" autoplay muted loop playsinline height="100%">
                <source src="video/mocap_comp_1.mp4" type="video/mp4">
              </video>
            </div>
          </div>
        </div>
      </div>
      <br>
      <p class="text-left">
        Envposer is validated on the GIMO and EgoBody datasets and demonstrated in a real-world PICO device.
      </p>
    </div>
  </section>	
	
<!-- citation -->
  <section>
  <div class="container">
    <div class="row ">
      <div class="col-12">
        <div class="x-section-title"><div class="x-gradient-font">Citation</div></div>
        <p class="bibtex x-gradient-block">
	
  @article{xia2024envposer,
  title={EnvPoser: Environment-aware Realistic Human Motion Estimation from Sparse Observations with Uncertainty Modeling},
  author={Xia, Songpengcheng and Zhang, Yu and Su, Zhuo and Zheng, Xiaozheng and Lv, Zheng and Wang, Guidong and Zhang, Yongjie and Wu, Qi and Chu, Lei and Pei, Ling},
  journal={ IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR)},
  year={2025},
  publisher={IEEE},
  booktitle={cvpr}
}
        </p>
      </div>
    </div>
  </div>
  </section>

  <!-- bottom bar -->
  <section>
  <div id="bottombar">
    <div><b>EnvPoser: Environment-aware Realistic Human Motion Estimation from Sparse Observations with Uncertainty Modeling</b></div>
    <div>Thanks to <a href="https://lioryariv.github.io/" target="_blank">Lior Yariv</a> and <a href="https://jeffreyxiang.github.io/" target="_blank">Jianfeng Xiang</a> for the website template</div>
  </div>
  </section>

</html>
